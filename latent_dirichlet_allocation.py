# -*- coding: utf-8 -*-
"""
Created on Thu Jun 22 06:46:27 2023

@author: yuabr
"""

#%%
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim

doc_a = 'Brocolli is good to eat. My brother likes to eat good brocolli, ' + \
    'but not my mother.'
doc_b = 'My mother spends a lot of time driving my brother around to baseball ' + \
    'practice.'
doc_c = 'Some health experts suggest that driving may cause increased tension ' + \
    'and blood pressure.'
doc_d = 'I often feel pressure to perform well at school, but my mother never ' + \
    'seems to drive my brother to do better.'
doc_e = 'Health professionals say that brocolli is good for your health.'

doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]

#%% Data cleaning

#Tokenizing: converting a document to its atomic elements. 
#Stopping: removing meaningless words. 
#Stemming: merging words that are equivalent in meaning.

texts = []

tokenizer = RegexpTokenizer(r'\w+')
#RegexpTokenizer(pattern, gaps = False, discard_empty = True)
#A tokenizer that splits a string using a regular experssion, which matches 
#either the tokens or the separators between tokens.
#pattern: str. The pattern used to build this tokenizer.
#gaps: True if this tokenizer's pattern should be used to find separators between 
#  tokens; False if this tokenizer's pattern should be used to find the tokens 
#  themselves.
#discard_empty: True if any empty tokens generated by the tokenizer should be 
#  discarded. Empty tokens can only be generated if `_gaps == True`.

#create English stop words list
en_stop = get_stop_words('en')
#get_stop_words(language)

#create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

for i in doc_set: 
    
    #break
    
    #clean and tokenize document string
    raw = i.lower()
    tokens = tokenizer.tokenize(raw)
    
    if doc_set.index(i) == 0: 
        print(tokens)
    
    #remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]
    #i in this comphrehension does not change the i in the outer for loop!
    
    if doc_set.index(i) == 0: 
        print(stopped_tokens)
    
    #stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    #i in this comphrehension does not change the i in the outer for loop!
    
    if doc_set.index(i) == 0: 
        print(stemmed_tokens)
    
    texts.append(stemmed_tokens)
    
#%% Constructing a document-term matrix

#turn the tokenized documents into an id <-> term dictionary
dictionary = corpora.Dictionary(texts)
print(dictionary.token2id)
#{'brocolli': 0, 'brother': 1, 'eat': 2, 'good': 3, 'like': 4, 'mother': 5, 
#'around': 6, 'basebal': 7, 'drive': 8, 'lot': 9, 'practic': 10, 'spend': 11, 
#'time': 12, 'blood': 13, 'caus': 14, 'expert': 15, 'health': 16, 'increas': 17, 
#'may': 18, 'pressur': 19, 'suggest': 20, 'tension': 21, 'better': 22, 
#'feel': 23, 'never': 24, 'often': 25, 'perform': 26, 'school': 27, 'seem': 28, 
#'well': 29, 'profession': 30, 'say': 31}
dictionary[0]
#'brocolli'
try: 
    dictionary['brocolli']
    #KeyError: 'brocolli'
except KeyError: 
    pass

#convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]
#dictionary.doc2bow(document)
#Convert `document` into the bag-of-words (BoW) format = list of `(token_id, 
#token_count)` tuples.
#document: list of str. Input document.
#doc2bow() only includes terms that actually occur: terms that do not occur in 
#a document will not appear in that document's vector.

print(corpus[0])
#[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]

#%% Applying the LDA model

#generate LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 2, 
                                           id2word = dictionary, passes = 20, 
                                           random_state = 2023)
#gensim.models.ldamodel.LdaModel(corpus = None, num_topics = 100, 
#  id2word = None, passes = 1, random_state = None, 
#  chunksize = 2000, update_every = 1)
#Train and use Online Latent Dirichlet Allocation model as presented in 
#`'Online Learning for LDA' by Hoffman et al.`_
#corpus: iterable of list of (int, float), optional.
#  Stream of document vectors or sparse matrix of shape (`num_documents`, 
#  `num_terms`). If not given, the model is left untrained (presumably because 
#  you want to call :meth:`~gensim.models.ldamodel.LdaModel.update` manually).
#num_topics: int, optinal. The number of requested latent topics to be extracted 
#  from the training corpus.
#id2word: {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}
#  Mapping from word IDs to words. It is used to determine the vocabulary size, 
#  as well as for debugging and topic printing.
#passes: int, optional. Number of passes through the corpus during training. The 
#  greater the number of passes, the more accurate the model will be. A lot of 
#  passes can be slow on a very large corpus.
#random_state: {np.random.RandomState, int}, optinal. Either a randomState object 
#  or a seed to generate one. Useful for reproducibility.
#chunksize: int, optinal. Number of documents to be used in each training chunk. 
#update_every: int, optional. Number of documents to be iterated through for each 
#  update. Set to 0 for batch learning, > 1 for online iterative learning.

#Adjusting the model's number of topics and passes is important to getting a good 
#result.

print(ldamodel.print_topics(num_topics = 2, num_words = 4))
#ldamodel.print_topics(num_topics = 20, num_words = 10)
#Get the most significant topics. 
#num_topics: int, optional. The number of topics to be selected, if -1 - all 
#  topics will be in result (ordered by significance). 
#num_words: int, optinal. The number of words to be included per topics (ordered 
#  by significance). 

#[(0, '0.086*"brocolli" + 0.086*"good" + 0.086*"health" + 0.061*"eat"'), 
#(1, '0.068*"brother" + 0.068*"mother" + 0.068*"drive" + 0.041*"pressur"')]

#Each generated topic is separated by a comma. Within each topic are the most 
#probable words to appear in that topic.

#Each topic contains all the words in the dictionary, when shown all the words 
#in that topic, so the words in different topics are the same. However, the 
#weights of the words in each topic are unique. And the weights for all the 
#words in a topic have a sum of 1.









    
