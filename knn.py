# -*- coding: utf-8 -*-
"""
Created on Tue Nov 03 21:03:51 2020

@author: abrah
"""

#%%
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt

from sklearn import tree
import bisect
import time

Idx = Series(range(1, 18))

density = Series([0.697, 0.774, 0.634, 0.608, 0.556, 
                  0.403, 0.481, 0.437, 0.666, 0.243, 
                  0.245, 0.343, 0.639, 0.657, 0.360, 
                  0.593, 0.719])

sugar_ratio = Series([0.460, 0.376, 0.264, 0.318, 0.215, 
                      0.237, 0.149, 0.211, 0.091, 0.267, 
                      0.057, 0.099, 0.161, 0.198, 0.370, 
                      0.042, 0.103])

watermelon = pd.concat([Idx, density, sugar_ratio], axis = 1)
#pd.concat(objs, axis = 0) Concatenate pandas objects along a particular axis 
#                          with optional set logic along the other axis.
#objs: a sequence or mapping of Seires, DataFrame, or Panel objects
#axis: The axis to concatenate along

watermelon.columns = ['Idx', 'density', 'sugar_ratio']

watermelon = watermelon.set_index(['Idx'], drop = True)
#Set the DataFrame index (row labels) using one or more existing columns. By 
#default yields a new object.

del Idx, density, sugar_ratio

label = Series([1, 1, 1, 1, 1, 
                1, 1, 1, 0, 0, 
                0, 0, 0, 0, 0, 
                0, 0])

label.index = watermelon.index
#Use label.index to rename indeces

watermelon['label'] = label
#If use the Series label to set the new column of the DataFrame watermelon, 
#the Series and the DataFrame should have the same index, and the elements in 
#the Series label will be transferred to the row of the DataFrame with the same 
#index. If some index values in the Series don't have coorespondence in the 
#DataFrame, it will be dropped; while if some index values in the DataFrame 
#don't have correspondence in the Series, it will be set as nan. Hence, to 
#make sure the Series label and the DataFrame watermelon have the same index, 
#use label.index to reindex it.

watermelon['label'] = watermelon['label'].map({1: 'good', 0: 'bad'})
#map(function, sequence) Return a list of the results of applying the function 
#                        to the ITEMs of the argument sequence

del label

test1 = DataFrame([[1, 0.698, 0.461, 'good']], \
                  columns = watermelon.columns.insert(0, 'Idx'))
#Use watermelon.columns.insert(loc, item) to make new Index object with new 
#item inserted at location loc.



test1 = test1.set_index(['Idx'], drop = True)
#Set the DataFrame index (row labels) using one or more existing columns. By 
#default yields a new object.

#%%
class Node(object): 
    def __init__(self):
        self.father = None
        self.left = None        #left is the branch consisting of samples 
                                #with an optimal classification feature value 
                                #less than split_point value
        self.right = None       #right is the branch consisting of samples 
                                #with an optimal classification feature value 
                                #greater than split_point value
        self.split_point = None #split_point is the optimal classification 
                                #value to discretize the continuous feature
        self.feature = None     #feature is the column index of feature 
                                #selected as the optimal classification feature
                             
    #During the process of writing a class, if want to test its methods being 
    #written, an annoyance is the error "NameError: name 'self' is not defined". 
    #To get a self object and use it conviently during method writing and testing, 
    #can write the __init__ method of the class first, and generate an object 
    #named 'self' using the temporary class containing the __init__ method only, 
    #like self = Node() here, and then this self can be used to test other 
    #methods need to be written.
    
    def __str__(self):
        return "feature: {}, split point: {}".format(self.feature, \
                         str(self.split_point))
        #Note, after the line continuation character "\", there should be NO BLANK
        
    @property
    def is_leaf(self): 
        return (self.left is None) and (self.right is None)
    
    @property
    def brother(self):
        if self.father is None:
            return None
        
        return self.father.left if (self.father.right is self) else self.father.right
    
class KNN(object): 
    def __init__(self, k = 3): 
        self.k = k 
        
        self.m = None    #m is the sample number
        self.n = None    #n is the feature number
        self.root = None #root is the k-d tree generated by the training data
    
    #During the process of writing a class, if want to test its methods being 
    #written, an annoyance is the error "NameError: name 'self' is not defined". 
    #To get a self obejct and use it conviently during method writing and testing, 
    #can write the __init__ method of the class first, and generate an object 
    #named 'self' using the temporary class containing the __init__ method only, 
    #like self = KNN(k = 3) here, and then this self can be used to test other 
    #methods need to be written.
    
    def _chooseFeatureToSplit(self, X):
        """
        Choose the optimal classification feature (continuous)
        @X: DataFrame, each row is a sample and each column is a feature. The 
            feature colunms should be continuous values
        """
        
        m_, n_ = X.shape
        
        #If X is empty, no need to select features
        if m_ == 0 or n_ == 0: 
        #or is a logical OR that returns True if any of the operands is true 
        #whereas '|' is a bitwise operater in Python that acts on bits and 
        #performs bit by bit operation.
            return None
                
        varlist = np.var(X)
        #np.var(a, axis = None) Compute the variance along the specified axis. 
        #                       The variance is computed for the flattened array 
        #                       by default, otherwise over the specified axis.
        #a: array_like
        #axis: Axis or axes along which the variance is computed. The default is 
        #      to compute the variance of the flattened array.
        
        selectedfeature = np.argmax(list(varlist))
        
        return selectedfeature
    
    def _generate_kd(self, X, father = None): 
        """
        Generate k-d tree via recursion
        @X: DataFrame, each row is a sample and each column is a feature, except 
            the last column, which is the sample labels. The feature columns 
            should be continuous values
        @father: father node of the k-d tree or k-d tree branch need to be 
                 generated
        """
        X.index = range(1, X.shape[0] + 1)
        
        y = X.ix[:,X.shape[1]-1]
        y = y.to_frame()
        #y.to_frame() Convert Series to DataFrame
        #The result DataFrame has 1 column and the same number of rows as the 
        #original Series
        
        #For y = X.ix[:, X.shape[1] - 1], y is only a Series, not a DataFrame. 
        #Use y.to_frame() to change it to a DataFrame.
        
        X = X.ix[:,0:X.shape[1]-1]
        #When X is a one-row DataFrame, if use X.ix[:,0:X.shape[1]-1] to select, 
        #the result is also a one-row DataFrame, but if use X.ix[1,0:X.shape[1]-1] 
        #to select, the result is a Series.
               
        m_, n_ = X.shape
        
        #If X is empty, the current node is None
        if m_ == 0 or n_ == 0:
        #or is a logical OR that returns True if any of the operands is true
        #whereas '|' is a bitwise operater in Python that acts on bits and 
        #performs bit by bit operation.            
            return None
        
        node = Node()
        node.father = father
        
        feature = self._chooseFeatureToSplit(X = X)
        node.feature = feature
        
        #If there is only one sample left, set it as leaf node directly
        if m_ == 1: 
            X.index = [1]
            #Use X.index to rename indeces
            
            y.index = [1]
            
            split_point = X
            split_point['label'] = y
            split_point.index = [1]
            
            node.split_point = split_point
            
            return node
            
            
        feature_rank = X.ix[:,feature].rank(method = 'first').astype(int)
        #rank(self, axis = 0, method = 'average', ascending = True)
        #    Compute numerical data ranks (1 through n) along axis. Equal values 
        #    are assigned a rank that is the average of the ranks of those values 
        #axis: {0 or 'index', 1 or 'columns'}, default 0 index to direct ranking 
        #method: {'average', 'min', 'max', 'first', 'dense'}
        #    *average: average rank of group
        #    *first: ranks assigned in order they appear in the array
        #ascending: boolean, default True False for ranks by high (1) to low (N)
        
        median_idx = feature_rank[feature_rank == m_ // 2].index[0]
        
        split_point = X.ix[median_idx,:]
        
        split_point = split_point.to_frame()
        #split_point.to_frame() Convert Series to DataFrame
        #The result DataFrame has 1 column and the same number of rows as the 
        #original Series
    
        split_point = split_point.T
        
        
        
        label = y.ix[median_idx,:]
        label = label.to_frame()
        label = label.T
               
        split_point['label'] = label
        split_point.index = [1]
        #Use split_point.index to rename indeces
        
        node.split_point = split_point
        
        left_idx = feature_rank < (m_ // 2)
        right_idx = feature_rank > (m_ // 2)
        
        X['lable'] = y
        
        watermelon_left = X.ix[left_idx, :]
        watermelon_right = X.ix[right_idx, :]
        watermelon_left.index = range(1, watermelon_left.shape[0] + 1)
        watermelon_right.index = range(1, watermelon_right.shape[0] + 1)
        
        #Generate branches
        node.left = self._generate_kd(X = watermelon_left, father = node)

        node.right = self._generate_kd(X = watermelon_right, father = node)
        
        return node
    
    
    def fit(self, X): 
        """
        Train a k-d tree
        @X: DataFrame, each row is a sample and each column is a feature, except 
            the last column, which is the sample labels. The feature columns 
            should be continuous values
        """
        
        self.m = X.shape[0]
        self.n = X.shape[1] - 1
        
        self.root = self._generate_kd(X = X)
        
        return self
    
    def _dist_to_node(self, x, node): 
        """
        Calculate the distance between the sample and the node
        @x: one-row DataFrame, the row is a sample and each column is a feature. 
            The features should be continuous values
        @node: the root node of a k-d tree or k-d tree branch
        """
        x.index = [1]
        
        splitnode = node.split_point
        
        splitnode = splitnode.ix[1,0:(splitnode.shape[1] - 1)]
        
        splitnode = splitnode.to_frame()
        #splitnode.to_frame() Convert Series to DataFrame
        #The result DataFrame has 1 column and the same number of rows as the 
        #original Series
        
        splitnode = splitnode.T
        
        x.columns = splitnode.columns
        #Because for the next command dist = np.linalg.norm(x - splitnode). For 
        #the substraction calculation between the two DataFrames x and splitnode, 
        #x and splitnode must have the same indeces AND COLUMN names, otherwise, 
        #the corresponding values in the DataFrames will not be considerred as 
        #corresonding and calcualted together and NaN values will be generated. 
        #Hence, for x, its COLUMN names need to be renamed using the COLUMN names 
        #of splitnode.    
        
        dist = np.linalg.norm(x - splitnode)
        #np.linalg.norm(x, ord = None, axis = None)
        #  This function is able to return one of eight different matrix norms, 
        #  or one of an infinite number of vector norms, depending on the value 
        #  of the ``ord`` parameter.
        #x: array_like Input array. If `axis` is None, `x` must be 1-D or 2-D.
        #ord: {non-zero int, inf, -inf, 'fro', 'nuc'}, optional Order of the norm.
        
        #For the calculation between DataFrames, for example, x - splitnode here, 
        #x and splitnode must have the same indeces AND COLUMN names, otherwise, 
        #the corresponding values in the DataFrames will not be 
        #considerred as corresonding and calcualted together and NaN 
        #values will be generated.
                
        return dist
        
    def _dist_to_hyperplane(self, x, node): 
        """
        Calculate the distance between the sample and the hyperplane of the node
        @x: one-row DataFrame, the row is a sample and each column is a feature. 
            The features should be continuous values
        @node: the root node of a k-d tree or k-d tree branch
        """
        x.index = [1]
        
        j = node.feature
        
        dist = abs(x.ix[1, j] - node.split_point.ix[1, j])
        
        return dist
       
    def _insert_node(self, x, node, knn_dist, knn_node): 
        """
        Insert the current root node of the k-d tree or k-d tree branch into 
        the existed k nearest neighbors of the sepcific sample to update the 
        k nearest neighbors
        @x: one-row DataFrame, the row is a sample and each column is a feature. 
            The features should be continuous values
        @node: the root node of a k-d tree or k-d tree branch
        @knn_dist: the existed distances of the k nearest nodes to the sample 
                   need to be updated
        @knn_node: the existed k nearest nodes to the sample need to be updated
        """
        
        dist = self._dist_to_node(x = x, node = node)
        
        idx = bisect.bisect(knn_dist, dist)
        #bisect(a, x) -> index Return the index where to insert item x in list a, 
        #                      assuming a is sorted.
        #                      The return value i is such that all e in a[:i] 
        #                      have e <= x, and all e in a[i:] have e > x. So 
        #                      if x already appears in the list, i points just 
        #                      beyond the rightmost x already there.
        
        
        #If knn_dist has less than k values, directly insert dist; 
        #if it has k values, and dist is less than the maximum value of knn_dist, 
        #insert dist and then drop the maximum value
        if len(knn_dist) < self.k: 
            
            knn_dist.insert(idx, dist)
            knn_node.insert(idx, node)
            
        elif knn_dist[-1] > dist: 
            
            knn_dist.insert(idx, dist)
            knn_node.insert(idx, node)
            
            knn_dist = knn_dist[:-1]
            knn_node = knn_node[:-1]
            
        return knn_dist, knn_node
    
    def _get_leaf(self, x, node):
        """
        Get the leaf node according to x and the branching of the k-d tree node 
        @x: one-row DataFrame, the row is a sample and each column is a feature. 
            The features should be continuous values
        @node: the root node of a k-d tree or k-d tree branch            
        """
        x.index = [1]
        
        while node.is_leaf is not True: 
            if node.left is None: 
                node = node.right
            elif node.right is None: 
                node = node.left
            else:
                if x.ix[1, node.feature] < node.split_point.ix[1, node.feature]: 
                    node = node.left
                else: 
                    node = node.right
        
        return node
        
    def _knn_search(self, x, nd_root = None, knn_dist = None, knn_node = None): 
        """
        Get the k nearest nodes for a specific sample using a recursion method 
        and will return these nearest nodes as well as their distances to the 
        sample
        @x: one-row DataFrame, the row is a sample and each column is a feature. 
            The features should be continuous values
        @nd_root: the root node of a k-d tree or k-d tree branch
        @knn_dist: the existed distances of the k nearest nodes to the sample 
                   need to be updated
        @knn_node: the existed k nearest nodes to the sample need to be updated
        """
        if knn_dist is None: 
            knn_dist = []
            knn_node = []
            
        knn_dist, knn_node = self._insert_node(x = x, node = nd_root, \
                                               knn_dist = knn_dist, \
                                               knn_node = knn_node)
        #Note, after the line continuation character "\", there should be NO BLANK
        
        nd_cur = self._get_leaf(x = x, node = nd_root)
        
        while nd_cur is not nd_root: 
            knn_dist, knn_node = self._insert_node(x = x, node = nd_cur, \
                                                   knn_dist = knn_dist, \
                                                   knn_node = knn_node)
            
            #If knn_dist and knn_node have less than k nodes, or the maximum 
            #distance in knn_dist is greater than the distance between the 
            #sample and the hyperplane of the father node, need to also 
            #check the distances between the sample and the brother node branch, 
            #and update knn_dist and knn_node
            if (nd_cur.brother is not None) and \
            ((len(knn_dist) < self.k) or (knn_dist[-1] > self._dist_to_hyperplane(x = x, \
               node = nd_cur.father))): 
            #or is a logical OR that returns True if any of the operands is true 
            #whereas '|' is a bitwise operater in Python that acts on bits and 
            #performs bit by bit operation.
                
                knn_dist, knn_node = self._knn_search(x = x, nd_root = nd_cur.brother, \
                                                      knn_dist = knn_dist, \
                                                      knn_node = knn_node)
            
            #Update nd_cur using its father node, to fulfill a backward tracking
            nd_cur = nd_cur.father
            
        return knn_dist, knn_node
       
    def predict(self, X): 
        """
        Predict the label of each sample included in DataFrame X, using KNN 
        method
        @X: DataFrame, each row is a sample and each column is a feature. The 
            feature columns should be continous values
        """
        X.index = range(1, X.shape[0] + 1)
        
        m = X.shape[0]
        result = np.empty((m,), dtype = int)
        #np.empty(shape, dtype = float)
        #  Return a new array of given shape and type, without initializing 
        #  entries (An array of arbitrary data of given shape and dtype).
        #shape: int or tuple of int Shape of the empty array
        #dtype: data-type, optional Desired output data-type.
        
        result = result.astype(dtype = \
                               type(self.root.split_point.ix[1, self.root.split_point.shape[1] - 1]))
        #Note, after the line continuation character "\", there should be NO BLANK

        for i in range(1, m + 1):
            
            x = X.ix[i,:]
            x = x.to_frame()
            x = x.T
            x.index = [1]
            
            _, knn_node = self._knn_search(x = x, nd_root = self.root)
            #Here, _ is a dummy variable
            
            result[i - 1] = np.argmax(Series([nd.split_point.ix[1, \
                  nd.split_point.shape[1] - 1] for nd in knn_node]).value_counts())
            #value_counts(values, sort = True) Compute a histogram of the counts 
            #                                  of non-null values
            #values: ndarray (1-d)
            #sort: boolean, default True. Sort by values
        
        return result
    
def plt_decision_bound(X_, classifier, title): 
    """
    Draw the decision boundary from the classifier
    @X_: DataFrame, each row is a sample and each column is a feature, except the 
         last column, which is the sample labels. The feature columns should be 
         continuous values
    @classifier: a classifier trained with predict method to predict the labels 
                 of samples
    @title: the title of the plot    
    """
    y_ = X_.ix[:,X_.shape[1]-1]
    X_ = X_.ix[:,0:X_.shape[1]-1]
    pos = y_ == 'good'
    neg = y_ == 'bad'
    
    x_tmp = np.linspace(0.1, 0.8, 100)
    y_tmp = np.linspace(-0.1, 0.55, 100)
    #numpy.linspace(start, stop, num = 50) Return evenly spaced numbers over a 
    #                                      specified interval. 
    #                                      Return num = 50 (or other) evenly 
    #                                      spaced samples, calculated over the 
    #                                      interval[start, stop]. 
    #                                      The endpoint of the interval can 
    #                                      optionally be excluded.
    
    X_tmp, Y_tmp = np.meshgrid(x_tmp, y_tmp)
    #numpy.meshgrid(*xi) Return coordinate matrices from coordinate vectors 
    #                    *xi are coordinate vectors and should be array-like. 
    #For example, x = np.array([1, 2, 3]), y = np.array([4, 5, 6, 7]), 
    #xv, yv = np.meshgrid(x, y). Then we can get 
    #xv is array([[1, 2, 3], 
    #             [1, 2, 3], 
    #             [1, 2, 3], 
    #             [1, 2, 3]])
    #yv is array([[4, 4, 4], 
    #             [5, 5, 5], 
    #             [6, 6, 6], 
    #             [7, 7, 7]])
    #The column number of xv = the column number of yv = length of x 
    #Each row of xv is x
    #The row number of xv = the row number of yv = length of y 
    #Each column of yv is y 
    #While if merge xv and yv together, can get 
    #array([[(1, 7), (2, 7), (3, 7)], 
    #       [(1, 6), (2, 6), (3, 6)], 
    #       [(1, 5), (2, 5), (3, 5)], 
    #       [(1, 4), (2, 4), (3, 4)]])
    #For this merged array, its shape is the same as xv and yv. The x coordinates 
    #of the dots are from xv while the y coordinates of the dots are from yv. 
    #For the coordinates of the dots, they will become the crosses of grid lines 
    #parallel to x axis and y axis in the coordinate system.
    
    tmp = DataFrame(np.c_[X_tmp.ravel(), Y_tmp.ravel()])
    #numpy.ravel Return a continuous flattened array
    #numpy.c_ Translates slice objects to concatenation along the second axis
    
    #tmp.columns = list(X_.columns)[0:tmp.shape[1]]
    #tmp.index = range(1, tmp.shape[0] + 1)
    #Because for the next command classifier.predict(tmp).reshape(X_tmp.shape), 
    #the predict method will be used, and it depends on the internal command 
    #dist = np.linalg.norm(x - splitnode). For the substraction calculation 
    #between the two DataFrames x and splitnode, x and splitnode must have the 
    #same indeces AND COLUMN names, otherwise, the corresponding values in the 
    #DataFrames will not be considerred as corresonding and calcualted together 
    #and NaN values will be generated. Because splitnode is included in the k-d 
    #tree trained, it has the index started from 1, and COLUMN names same as 
    #the training dataset X_, for tmp, which will serve as x during 
    #the calculation, its index and COLUMN names need to be renamed using 
    #a range from 1 and the COLUMN names of X_ respectively.
    
    Z_ = classifier.predict(tmp).reshape(X_tmp.shape)
    #reshape return an NDARRAY with the values shape
    
    Z_ = DataFrame(Z_)
    Z_ = Z_.replace(to_replace = ['good', 'bad'], value = [1, -1])
    #DataFrame.replace(to_replace = None, value = None)
    #    Replace values given in 'to_replace' with 'value'
    #    to_replace: str, regex, list, dict, Series, numeric, or None
    #    value: scalar, dict, list, str, regex, default None
    
    plt.contour(X_tmp, Y_tmp, Z_, [0], colors = 'orange', linewidths = 1)
    #plt.contour(X, Y, Z, levels, **kwargs) Draw contour lines. 
    #    X, Y: array-like, optional. The coordinates of the values in Z. X and Y 
    #          must both be 2-D with the same shape as Z (e.g. created via 
    #          numpy.meshgrid), or they must both be 1-D such that len(X) == M 
    #          is the number of columns in Z and len(Y) == N is the number of 
    #          rows in Z.
    #    Z: array-like(N, M) The height values over which the contour is drawn. 
    #    levels: int or array-like, optional. Determines the number and positions 
    #            of the contour lines/regions. If an int n, use n data intervals; 
    #            i.e. draw n+1 contour lines. The level heights are automatically 
    #            chosen. If array-like, draw contour lines at the specific levels, 
    #            The values must be in increasing order.
    
    plt.scatter(X_.ix[pos, 0], X_.ix[pos, 1], label = '1', color = 'c')
    plt.scatter(X_.ix[neg, 0], X_.ix[neg, 1], label = '0', color = 'lightcoral')
    #plt.scatter(x, y) Make a scatter plot of `x` vs `y`
    #    x, y: array_like, shape(n,)
    
    plt.title(title)
    #plt.title Set a title of the current axes. 
    #Set one of the three available axes titles. The available titles are 
    #positioned above the axes in the center, flush with the left edge, 
    #and flush with the right edge.
    
    plt.legend()
    #plt.legend Place a legend on the axes. 
    #This method can automatically detect the elements to be shown in the legend 
    #The elements to be added to the legend are automatically determined, when 
    #you do not pass in any extra arguments. 
    #In this case, the LABELs (LABEL = '1' here) are taken from the 
    #artist(plt.scatter(X_.ix[pos, 0], X_.ix[pos, 1], LABEL = '1', color = 'c') 
    #here). You can specify them either at artist creation or by calling the 
    #set_label() method on the artist. 
    plt.show()
    #plt.show Display a figure. When running in ipython with its pylab mode, 
    #         display all figures and return to the ipython prompt. 

#%%
if __name__ == '__main__': 
    knn = KNN(k = 3)
    
    #Decision bound of KNN
    knn.fit(X = watermelon)
    
    plt_decision_bound(X_ = watermelon, classifier = knn, \
                       title = 'decision bound of knn')
    
    #Decision bound of decision tree
    ds_tree = tree.DecisionTreeClassifier()
    #tree.DecisionTreeClassifier(self, criterion = 'gini')
    #    A decision tree classifier.
    
    X = watermelon.ix[:,0:watermelon.shape[1] - 1]
    y = watermelon.ix[:,watermelon.shape[1] - 1]
    
    ds_tree = ds_tree.fit(X, y)
    
    plt_decision_bound(X_ = watermelon, classifier = ds_tree, \
                       title = 'decision bound of decision tree')
    
    
    
    
    
    t = np.random.randint(0, 500, (100, 3))
    #np.random.randint(low, high = None, size = None) 
    #    Return random integers from `low` (inclusive) to `high` (exclusive). 
    #    Return random integers from the "discrete uniform" distribution of 
    #    the specified dtype in the "half-open" interval [`low`, `high`). If 
    #    `high` is None (the default), then results are from [0, `low`).
    #size: int or tuple of ints, optional
    #  Output shape. If the given shape is, e.g., ``(m, n, k)``, then 
    #  ``m * n * k`` samples are drawn. Default is None, in which case a 
    #  single value is returned.
    
    t = DataFrame(t)
    t.index = range(1, t.shape[0] + 1)
    t.columns = ['density', 'sugar_ratio', 'label']
    
    knn.fit(X = t)
    
    te = np.random.randint(0, 500, (1, 2))
    #np.random.randint(low, high = None, size = None)
    #    Return random integers from `low` (inclusive) to `high` (exclusive). 
    #    Return random integers from the "discrete uniform" distribution of 
    #    the specified dtype in the "half-open" interval [`low`, `high`). If 
    #    `high` is None (the default), then results are from [0, `low`). 
    #size: int or tuple of ints, optional 
    #  Output shape. If the given shape is, e.g., ``(m, n, k)``, then 
    #  ``m * n* k`` samples are drawn. Default is None, in which case a 
    #  single value is returned.    
    
    te = DataFrame(te)
    te.index = [1]
    te.columns = ['density', 'sugar_ratio']
    
    knn_dist, knn_node = knn._knn_search(x = te, nd_root = knn.root)
    
    te = te.ix[1,:]
    e = np.linalg.norm(t.ix[:,0:t.shape[1]-1] - te, axis = 1)
    #The shape of the DataFrame t.ix[:,0:t.shape[1]-1] is (100, 2), while for 
    #the DataFrame te, its shape is (1, 2). However, to archieve a broadcast 
    #to substract te from each row of t, te must be converted to a Series using 
    #the command te = te.ix[1,:], which will generate a Series, not a DataFrame.
    
    #For the calculation between the DataFrame t.ix[:,0:t.shape[1]-1] and the 
    #Series te, the column names of the DataFrame must be same to the 
    #index of the Series, so that the corresponding values with the same names 
    #(column names of the DataFrame and index names of the Series) will be 
    #considered as corresponding. Otherwise, they will not be calculated 
    #together and NaN values will be generated.
    
    #np.linalg.norm(x, ord = None, axis = None)
    #  This function is able to return one of eight different matrix norms, 
    #  or one of an infinite number of vector norms, depending on the value 
    #  of the ``ord`` parameter. 
    #x: array_like Input array. If `axis` is None, `x` must be 1-D or 2-D. 
    #ord: {non-zero int, inf, -inf, 'fro', 'nuc'}, optional Order of the norm.
   
    e.sort(axis = 0)
    #sort(axis = -1) Sort an array, in-place. 
    #axis: int, optional Axis along which to sort. Default is -1, which means 
    #      sort along the last axis.
    
    e[0:len(knn_dist)] == knn_dist
   
#%%
%reset
    

